{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facial_keypoints",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armaan-20/Facial_keypoints/blob/main/facial_keypoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzb3tHrGYH6N"
      },
      "source": [
        "# import the required libraries\r\n",
        "import glob\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "\r\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGcA3JiibETS"
      },
      "source": [
        "key_pts_frame = pd.read_csv('P1_Facial_Keypoints/data/training_frames_keypoints.csv')\r\n",
        "\r\n",
        "n = 0\r\n",
        "image_name = key_pts_frame.iloc[n, 0]\r\n",
        "key_pts = key_pts_frame.iloc[n, 1:].to_numpy()\r\n",
        "key_pts = key_pts.astype('float').reshape(-1, 2)\r\n",
        "\r\n",
        "print('Image name: ', image_name)\r\n",
        "print('Landmarks shape: ', key_pts.shape)\r\n",
        "print('First 4 key pts: {}'.format(key_pts[:4]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1_EyhYhbL0d"
      },
      "source": [
        "print('Number of images: ', key_pts_frame.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHEnAAoTbbAQ"
      },
      "source": [
        "def show_keypoints(image, key_pts):\r\n",
        "    \"\"\"Show image with keypoints\"\"\"\r\n",
        "    plt.imshow(image)\r\n",
        "    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stdJ4Iywbk8t"
      },
      "source": [
        "n = 0\r\n",
        "image_name = key_pts_frame.iloc[n, 0]\r\n",
        "key_pts = key_pts_frame.iloc[n, 1:].to_numpy()\r\n",
        "key_pts = key_pts.astype('float').reshape(-1, 2)\r\n",
        "\r\n",
        "plt.figure(figsize=(5, 5))\r\n",
        "show_keypoints(mpimg.imread(os.path.join('P1_Facial_Keypoints/data/training/', image_name)), key_pts)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV7DErsrbs1W"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "class FacialKeypointsDataset(Dataset):\r\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.key_pts_frame = pd.read_csv(csv_file)\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.key_pts_frame)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        image_name = os.path.join(self.root_dir,\r\n",
        "                                self.key_pts_frame.iloc[idx, 0])\r\n",
        "        \r\n",
        "        image = mpimg.imread(image_name)\r\n",
        "        \r\n",
        "        # if image has an alpha color channel, get rid of it\r\n",
        "        if(image.shape[2] == 4):\r\n",
        "            image = image[:,:,0:3]\r\n",
        "        \r\n",
        "        key_pts = self.key_pts_frame.iloc[idx, 1:].to_numpy()\r\n",
        "        key_pts = key_pts.astype('float').reshape(-1, 2)\r\n",
        "        sample = {'image': image, 'keypoints': key_pts}\r\n",
        "\r\n",
        "        if self.transform:\r\n",
        "            sample = self.transform(sample)\r\n",
        "\r\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDnCrd8xbyqo"
      },
      "source": [
        "# Construct the dataset\r\n",
        "face_dataset = FacialKeypointsDataset(csv_file='P1_Facial_Keypoints/data/training_frames_keypoints.csv',\r\n",
        "                                      root_dir='P1_Facial_Keypoints/data/training/')\r\n",
        "\r\n",
        "# print some stats about the dataset\r\n",
        "print('Length of dataset: ', len(face_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVXAcN_Nb9uX"
      },
      "source": [
        "# Display a few of the images from the dataset\r\n",
        "num_to_display = 3\r\n",
        "\r\n",
        "for i in range(num_to_display):\r\n",
        "    \r\n",
        "    # define the size of images\r\n",
        "    fig = plt.figure(figsize=(20,10))\r\n",
        "    \r\n",
        "    # randomly select a sample\r\n",
        "    rand_i = np.random.randint(0, len(face_dataset))\r\n",
        "    sample = face_dataset[rand_i]\r\n",
        "\r\n",
        "    # print the shape of the image and keypoints\r\n",
        "    print(i, sample['image'].shape, sample['keypoints'].shape)\r\n",
        "\r\n",
        "    ax = plt.subplot(1, num_to_display, i + 1)\r\n",
        "    ax.set_title('Sample #{}'.format(i))\r\n",
        "    \r\n",
        "    # Using the same display function, defined earlier\r\n",
        "    show_keypoints(sample['image'], sample['keypoints'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnTp9idBcEeo"
      },
      "source": [
        "import torch\r\n",
        "from torchvision import transforms, utils\r\n",
        "# tranforms\r\n",
        "\r\n",
        "class Normalize(object):\r\n",
        "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"        \r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, key_pts = sample['image'], sample['keypoints']\r\n",
        "        \r\n",
        "        image_copy = np.copy(image)\r\n",
        "        key_pts_copy = np.copy(key_pts)\r\n",
        "\r\n",
        "        # convert image to grayscale\r\n",
        "        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\r\n",
        "        \r\n",
        "        # scale color range from [0, 255] to [0, 1]\r\n",
        "        image_copy=  image_copy/255.0\r\n",
        "        \r\n",
        "        # scale keypoints to be centered around 0 with a range of [-1, 1]\r\n",
        "        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\r\n",
        "        key_pts_copy = (key_pts_copy - 100)/50.0\r\n",
        "\r\n",
        "\r\n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy}\r\n",
        "\r\n",
        "\r\n",
        "class Rescale(object):\r\n",
        "    \"\"\"Rescale the image in a sample to a given size.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\r\n",
        "            matched to output_size. If int, smaller of image edges is matched\r\n",
        "            to output_size keeping aspect ratio the same.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, output_size):\r\n",
        "        assert isinstance(output_size, (int, tuple))\r\n",
        "        self.output_size = output_size\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, key_pts = sample['image'], sample['keypoints']\r\n",
        "\r\n",
        "        h, w = image.shape[:2]\r\n",
        "        if isinstance(self.output_size, int):\r\n",
        "            if h > w:\r\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\r\n",
        "            else:\r\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\r\n",
        "        else:\r\n",
        "            new_h, new_w = self.output_size\r\n",
        "\r\n",
        "        new_h, new_w = int(new_h), int(new_w)\r\n",
        "\r\n",
        "        img = cv2.resize(image, (new_w, new_h))\r\n",
        "        \r\n",
        "        # scale the pts, too\r\n",
        "        key_pts = key_pts * [new_w / w, new_h / h]\r\n",
        "\r\n",
        "        return {'image': img, 'keypoints': key_pts}\r\n",
        "\r\n",
        "\r\n",
        "class RandomCrop(object):\r\n",
        "    \"\"\"Crop randomly the image in a sample.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\r\n",
        "            is made.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, output_size):\r\n",
        "        assert isinstance(output_size, (int, tuple))\r\n",
        "        if isinstance(output_size, int):\r\n",
        "            self.output_size = (output_size, output_size)\r\n",
        "        else:\r\n",
        "            assert len(output_size) == 2\r\n",
        "            self.output_size = output_size\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, key_pts = sample['image'], sample['keypoints']\r\n",
        "\r\n",
        "        h, w = image.shape[:2]\r\n",
        "        new_h, new_w = self.output_size\r\n",
        "\r\n",
        "        top = np.random.randint(0, h - new_h)\r\n",
        "        left = np.random.randint(0, w - new_w)\r\n",
        "\r\n",
        "        image = image[top: top + new_h,\r\n",
        "                      left: left + new_w]\r\n",
        "\r\n",
        "        key_pts = key_pts - [left, top]\r\n",
        "\r\n",
        "        return {'image': image, 'keypoints': key_pts}\r\n",
        "\r\n",
        "\r\n",
        "class ToTensor(object):\r\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        image, key_pts = sample['image'], sample['keypoints']\r\n",
        "         \r\n",
        "        # if image has no grayscale color channel, add one\r\n",
        "        if(len(image.shape) == 2):\r\n",
        "            # add that third color dim\r\n",
        "            image = image.reshape(image.shape[0], image.shape[1], 1)\r\n",
        "            \r\n",
        "        # swap color axis because\r\n",
        "        # numpy image: H x W x C\r\n",
        "        # torch image: C X H X W\r\n",
        "        image = image.transpose((2, 0, 1))\r\n",
        "        \r\n",
        "        return {'image': torch.from_numpy(image),\r\n",
        "                'keypoints': torch.from_numpy(key_pts)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fps7H5AZcK9J"
      },
      "source": [
        "# test out some of these transforms\r\n",
        "rescale = Rescale(100)\r\n",
        "crop = RandomCrop(50)\r\n",
        "composed = transforms.Compose([Rescale(250),\r\n",
        "                               RandomCrop(224)])\r\n",
        "\r\n",
        "# apply the transforms to a sample image\r\n",
        "test_num = 450\r\n",
        "sample = face_dataset[test_num]\r\n",
        "\r\n",
        "\r\n",
        "fig = plt.figure()\r\n",
        "for i, tx in enumerate([rescale, crop, composed]):\r\n",
        "    transformed_sample = tx(sample)\r\n",
        "\r\n",
        "    ax = plt.subplot(1, 3, i + 1)\r\n",
        "    plt.tight_layout()\r\n",
        "    ax.set_title(type(tx).__name__)\r\n",
        "    show_keypoints(transformed_sample['image'], transformed_sample['keypoints'])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-5HujWcO37"
      },
      "source": [
        "# define the data tranform\r\n",
        "# order matters! i.e. rescaling should come before a smaller crop\r\n",
        "data_transform = transforms.Compose([Rescale(250),\r\n",
        "                                     RandomCrop(224),\r\n",
        "                                     Normalize(),\r\n",
        "                                     ToTensor()])\r\n",
        "\r\n",
        "# create the transformed dataset\r\n",
        "transformed_dataset = FacialKeypointsDataset(csv_file='P1_Facial_Keypoints/data/training_frames_keypoints.csv',\r\n",
        "                                             root_dir='P1_Facial_Keypoints/data/training/',\r\n",
        "                                             transform=data_transform)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SSGqj3_cXzZ"
      },
      "source": [
        "# print some stats about the transformed data\r\n",
        "print('Number of images: ', len(transformed_dataset))\r\n",
        "\r\n",
        "# make sure the sample tensors are the expected size\r\n",
        "for i in range(5):\r\n",
        "    sample = transformed_dataset[i]\r\n",
        "    print(i, sample['image'].size(), sample['keypoints'].size())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbspTJ9ceu3"
      },
      "source": [
        "# import the usual resources\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# watch for any changes in model.py, if it changes, re-load it automatically\r\n",
        "%load_ext autoreload\r\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QILYJrJUcgTx"
      },
      "source": [
        "## TODO: Define the Net in models.py\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "\r\n",
        "## TODO: Once you've define the network, you can instantiate it\r\n",
        "# one example conv layer has been provided for you\r\n",
        "from models import Net\r\n",
        "\r\n",
        "net = Net()\r\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_oBXMlqcyom"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchvision import transforms, utils\r\n",
        "\r\n",
        "# the dataset we created in Notebook 1 is copied in the helper file `data_load.py`\r\n",
        "from data_load import FacialKeypointsDataset\r\n",
        "# the transforms we defined in Notebook 1 are in the helper file `data_load.py`\r\n",
        "from data_load import Rescale, RandomCrop, Normalize, ToTensor\r\n",
        "\r\n",
        "\r\n",
        "## TODO: define the data_transform using transforms.Compose([all tx's, . , .])\r\n",
        "# order matters! i.e. rescaling should come before a smaller crop\r\n",
        "data_transform = transforms.Compose([Rescale(250),\r\n",
        "                                     RandomCrop(224),\r\n",
        "                                     Normalize(),\r\n",
        "                                     ToTensor()])\r\n",
        "\r\n",
        "\r\n",
        "# testing that you've defined a transform\r\n",
        "assert(data_transform is not None), 'Define a data_transform'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_zmsd7c203"
      },
      "source": [
        "# create the transformed dataset\r\n",
        "transformed_dataset = FacialKeypointsDataset(csv_file='P1_Facial_Keypoints/data/training_frames_keypoints.csv',\r\n",
        "                                             root_dir='P1_Facial_Keypoints/data/training/',\r\n",
        "                                             transform=data_transform)\r\n",
        "\r\n",
        "print('Number of images: ', len(transformed_dataset))\r\n",
        "\r\n",
        "# iterate through the transformed dataset and print some stats about the first few samples\r\n",
        "for i in range(4):\r\n",
        "    sample = transformed_dataset[i]\r\n",
        "    print(i, sample['image'].size(), sample['keypoints'].size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAm6zWCGdElK"
      },
      "source": [
        "# load training data in batches\r\n",
        "batch_size = 10\r\n",
        "\r\n",
        "train_loader = DataLoader(transformed_dataset, \r\n",
        "                          batch_size=batch_size,\r\n",
        "                          \r\n",
        "                          shuffle=True, \r\n",
        "                          num_workers=4)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TuM0wWRdxRH"
      },
      "source": [
        "# load in the test data, using the dataset class\r\n",
        "\r\n",
        "# AND apply the data_transform you defined above\r\n",
        "\r\n",
        "# create the test dataset\r\n",
        "test_dataset = FacialKeypointsDataset(csv_file='P1_Facial_Keypoints/data/test_frames_keypoints.csv',\r\n",
        "                                             root_dir='P1_Facial_Keypoints/data/test/',\r\n",
        "                                             transform=data_transform)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abBfCxzeMpO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOuMRLod7ji"
      },
      "source": [
        "# load test data in batches\r\n",
        "batch_size = 10\r\n",
        "\r\n",
        "test_loader = DataLoader(test_dataset, \r\n",
        "                          batch_size=batch_size,\r\n",
        "                          shuffle=True, \r\n",
        "                          num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdgN-SoHeGNY"
      },
      "source": [
        "# test the model on a batch of test images\r\n",
        "\r\n",
        "def net_sample_output():\r\n",
        "    \r\n",
        "    # iterate through the test dataset\r\n",
        "    for i, sample in enumerate(test_loader):\r\n",
        "        \r\n",
        "        # get sample data: images and ground truth keypoints\r\n",
        "        images = sample['image']\r\n",
        "        key_pts = sample['keypoints']\r\n",
        "\r\n",
        "        # convert images to FloatTensors\r\n",
        "        images = images.type(torch.FloatTensor)\r\n",
        "        # forward pass to get net output\r\n",
        "        output_pts = net(images)\r\n",
        "        # reshape to batch_size x 68 x 2 pts\r\n",
        "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\r\n",
        "        \r\n",
        "        # break after first image is tested\r\n",
        "        if i == 0:\r\n",
        "            return images, output_pts, key_pts\r\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sf0uY-celIN"
      },
      "source": [
        "# call the above function\r\n",
        "# returns: test images, test predicted keypoints, test ground truth keypoints\r\n",
        "test_images, test_outputs, gt_pts = net_sample_output()\r\n",
        "\r\n",
        "# print out the dimensions of the data to see if they make sense\r\n",
        "print(test_images.data.size())\r\n",
        "print(test_outputs.data.size())\r\n",
        "print(gt_pts.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOFQJ8_Qetf4"
      },
      "source": [
        "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\r\n",
        "    \"\"\"Show image with predicted keypoints\"\"\"\r\n",
        "    # image is grayscale\r\n",
        "    plt.imshow(image, cmap='gray')\r\n",
        "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\r\n",
        "    # plot ground truth points as green pts\r\n",
        "    if gt_pts is not None:\r\n",
        "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-H-eyFTeveD"
      },
      "source": [
        "# visualize the output\r\n",
        "# by default this shows a batch of 10 images\r\n",
        "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\r\n",
        "\r\n",
        "    for i in range(batch_size):\r\n",
        "        plt.figure(figsize=(20,10))\r\n",
        "        ax = plt.subplot(1, batch_size, i+1)\r\n",
        "\r\n",
        "        # un-transform the image data\r\n",
        "        image = test_images[i].data   # get the image from it's wrapper\r\n",
        "        image = image.cpu().numpy()   # convert to numpy array from a Tensor\r\n",
        "        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\r\n",
        "\r\n",
        "        # un-transform the predicted key_pts data\r\n",
        "        predicted_key_pts = test_outputs[i].data\r\n",
        "        predicted_key_pts = predicted_key_pts.cpu().numpy()\r\n",
        "        # undo normalization of keypoints  \r\n",
        "        predicted_key_pts = predicted_key_pts*50.0+100\r\n",
        "        \r\n",
        "        \r\n",
        "        # plot ground truth points for comparison, if they exist\r\n",
        "        ground_truth_pts = None\r\n",
        "        if gt_pts is not None:\r\n",
        "            ground_truth_pts = gt_pts[i]         \r\n",
        "            ground_truth_pts = ground_truth_pts*50.0+100\r\n",
        "        \r\n",
        "        # call show_all_keypoints\r\n",
        "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\r\n",
        "            \r\n",
        "        plt.axis('off')\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "# call it\r\n",
        "visualize_output(test_images, test_outputs, gt_pts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BZeyv4me46y"
      },
      "source": [
        "## TODO: Define the loss and optimization\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "criterion = nn.MSELoss()\r\n",
        "\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "  net = net.cuda()\r\n",
        "  criterion = criterion.cuda()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl5OTxjCe9fF"
      },
      "source": [
        "def train_net(n_epochs):\r\n",
        "\r\n",
        "    # prepare the net for training\r\n",
        "    net.train()\r\n",
        "    training_loss = []\r\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\r\n",
        "        \r\n",
        "        running_loss = 0.0\r\n",
        "\r\n",
        "        # train on batches of data, assumes you already have train_loader\r\n",
        "        for batch_i, data in enumerate(train_loader):\r\n",
        "            # get the input images and their corresponding labels\r\n",
        "            images = data['image']\r\n",
        "            key_pts = data['keypoints']\r\n",
        "\r\n",
        "            # flatten pts\r\n",
        "            key_pts = key_pts.view(key_pts.size(0), -1)\r\n",
        "\r\n",
        "            # convert variables to floats for regression loss\r\n",
        "            key_pts = key_pts.type(torch.FloatTensor)\r\n",
        "            key_pts = key_pts.cuda()\r\n",
        "            images = images.type(torch.FloatTensor)\r\n",
        "            images = images.cuda()\r\n",
        "\r\n",
        "            # forward pass to get outputs\r\n",
        "            output_pts = net(images)\r\n",
        "\r\n",
        "            # calculate the loss between predicted and target keypoints\r\n",
        "            loss = criterion(output_pts, key_pts)\r\n",
        "\r\n",
        "            # zero the parameter (weight) gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "            \r\n",
        "            # backward pass to calculate the weight gradients\r\n",
        "            loss.backward()\r\n",
        "\r\n",
        "            # update the weights\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            # print loss statistics\r\n",
        "            # to convert loss into a scalar and add it to the running_loss, use .item()\r\n",
        "            running_loss += loss.item()\r\n",
        "            if batch_i % 10 == 9:    # print every 10 batches\r\n",
        "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/1000))\r\n",
        "                running_loss = 0.0\r\n",
        "        training_loss.append(running_loss)\r\n",
        "\r\n",
        "    print('Finished Training')\r\n",
        "    return training_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOrZjGpSfBFd"
      },
      "source": [
        "# train your network\r\n",
        "n_epochs = 100\r\n",
        "# start small, and increase when you've decided on your model structure and hyperparams\r\n",
        "\r\n",
        "training_loss = train_net(n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA6h1cHUk6gg"
      },
      "source": [
        "plt.figure()\r\n",
        "plt.semilogy(training_loss)\r\n",
        "plt.grid()\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Loss');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hWaAHvg5Mt"
      },
      "source": [
        "# test the model on a batch of test images\r\n",
        "\r\n",
        "def net_sample_output():\r\n",
        "    \r\n",
        "    # iterate through the test dataset\r\n",
        "    for i, sample in enumerate(test_loader):\r\n",
        "        \r\n",
        "        # get sample data: images and ground truth keypoints\r\n",
        "        images = sample['image']\r\n",
        "        key_pts = sample['keypoints']\r\n",
        "\r\n",
        "        # convert images to FloatTensors\r\n",
        "        images = images.type(torch.FloatTensor)\r\n",
        "        images = images.cuda()\r\n",
        "        # forward pass to get net output\r\n",
        "        output_pts = net(images)\r\n",
        "        # reshape to batch_size x 68 x 2 pts\r\n",
        "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\r\n",
        "        \r\n",
        "        # break after first image is tested\r\n",
        "        if i == 0:\r\n",
        "            return images, output_pts, key_pts\r\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mixXegOPhWWC"
      },
      "source": [
        "# get a sample of test data again\r\n",
        "test_images, test_outputs, gt_pts = net_sample_output()\r\n",
        "\r\n",
        "print(test_images.data.size())\r\n",
        "print(test_outputs.data.size())\r\n",
        "print(gt_pts.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnMMagQkr0_Z"
      },
      "source": [
        "## TODO: visualize your test output\r\n",
        "# you can use the same function as before, by un-commenting the line below:\r\n",
        "\r\n",
        "visualize_output(test_images, test_outputs, gt_pts)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3L74zkZvqph"
      },
      "source": [
        "## TODO: change the name to something uniqe for each new model\r\n",
        "model_dir = 'P1_Facial_Keypoints/saved_models/'\r\n",
        "model_name = 'keypoints_model_1.pt'\r\n",
        "\r\n",
        "# after training, save your model parameters in the dir 'saved_models'\r\n",
        "torch.save(net.state_dict(), model_dir+model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X53RSPgUwu7G"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WaJto7Tw9To"
      },
      "source": [
        "import cv2\r\n",
        "## load in color image for face detection\r\n",
        "image = cv2.imread('family_1.png')\r\n",
        "\r\n",
        "# switch red and blue color channels \r\n",
        "# --> by default OpenCV assumes BLUE comes first, not RED as in many images\r\n",
        "gray = image.copy()\r\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n",
        "print(image.shape)\r\n",
        "\r\n",
        "# plot the image\r\n",
        "fig = plt.figure(figsize=(9,9))\r\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL77O7qKylvT"
      },
      "source": [
        "# load in a haar cascade classifier for detecting frontal faces\r\n",
        "face_cascade = cv2.CascadeClassifier('P1_Facial_Keypoints/detector_architectures/haarcascade_frontalface_default.xml')\r\n",
        "\r\n",
        "# load in a haar cascade classifier for detecting eyes\r\n",
        "eye_cascade = cv2.CascadeClassifier('P1_Facial_Keypoints/detector_architectures/haarcascade_eye.xml')\r\n",
        "\r\n",
        "# run the detector\r\n",
        "# the output here is an array of detections; the corners of each detection box\r\n",
        "# if necessary, modify these parameters until you successfully identify every face in a given image\r\n",
        "gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\r\n",
        "faces = face_cascade.detectMultiScale(gray, 1.3, 12)\r\n",
        "\r\n",
        "# make a copy of the original image to plot detections on\r\n",
        "image_with_detections = image.copy()\r\n",
        "\r\n",
        "# loop over the detected faces, mark the image where each face is found\r\n",
        "for (x,y,w,h) in faces:\r\n",
        "    # draw a rectangle around each detected face\r\n",
        "    # you may also need to change the width of the rectangle drawn depending on image resolution\r\n",
        "    cv2.rectangle(image_with_detections,(x,y),(x+w,y+h),(255,0,0),2) #2\r\n",
        "    \r\n",
        "    # draw a rectangle around each detected eyes\r\n",
        "    roi_gray = gray[y:y+h, x:x+w]\r\n",
        "    roi_color = image_with_detections[y:y+h, x:x+w]\r\n",
        "    eyes = eye_cascade.detectMultiScale(roi_gray)\r\n",
        "    \r\n",
        "    for (ex,ey,ew,eh) in eyes:\r\n",
        "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(9,9))\r\n",
        "\r\n",
        "plt.imshow(image_with_detections)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU2vzS9z2qHu"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.autograd import Variable\r\n",
        "from models import Net\r\n",
        "\r\n",
        "net = Net()\r\n",
        "\r\n",
        "## load the best saved model parameters (by your path name)\r\n",
        "## You'll need to un-comment the line below and add the correct name for *your* saved model\r\n",
        "net.load_state_dict(torch.load('P1_Facial_Keypoints/saved_models/keypoints_model_1.pt'))\r\n",
        "\r\n",
        "## print out your net and prepare it for testing (uncomment the line below)\r\n",
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkSj5k7W3ANA"
      },
      "source": [
        "def show_all_keypoints(image, keypoints):  \r\n",
        "    batch_size = len(image)\r\n",
        "    for i, face in enumerate(image):\r\n",
        "        plt.figure(figsize=(8, 8))\r\n",
        "        ax = plt.subplot(1, batch_size, i+1)\r\n",
        "\r\n",
        "        # un-transform the predicted key_pts data\r\n",
        "        predicted_keypoints = keypoints[i].data\r\n",
        "        predicted_keypoints = predicted_keypoints.numpy()\r\n",
        "        # undo normalization of keypoints  \r\n",
        "        predicted_keypoints = predicted_keypoints*50.0+100\r\n",
        "\r\n",
        "        plt.imshow(face, cmap='gray')\r\n",
        "        plt.scatter(predicted_keypoints[:, 0], predicted_keypoints[:, 1], s=20, marker='.', c='m')\r\n",
        "        \r\n",
        "        plt.axis('off')\r\n",
        "\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjbC4vsV3FEF"
      },
      "source": [
        "image_copy = np.copy(image)\r\n",
        "#Including a padding to extract face as  HAAR classifier's bounding box, crops sections of the face\r\n",
        "\r\n",
        "PADDING = 40\r\n",
        "images, keypoints = [], []\r\n",
        "\r\n",
        "# loop over the detected faces from your haar cascade\r\n",
        "for (x,y,w,h) in faces:\r\n",
        "    \r\n",
        "    # Select the region of interest that is the face in the image \r\n",
        "    roi = image_copy[y-PADDING:y+h+PADDING, x-PADDING:x+w+PADDING]\r\n",
        "    \r\n",
        "    ## Convert the face region from RGB to grayscale\r\n",
        "    roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\r\n",
        "    \r\n",
        "    ## Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\r\n",
        "    roi = (roi / 255.).astype(np.float32)\r\n",
        "    \r\n",
        "    ## Rescale the detected face to be the expected square size for your CNN (224x224, suggested)\r\n",
        "    roi = cv2.resize(roi, (224, 224))\r\n",
        "    images.append(roi)\r\n",
        "    \r\n",
        "    ## Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\r\n",
        "    if len(roi.shape) == 2:\r\n",
        "        roi = np.expand_dims(roi, axis=0)\r\n",
        "    else:\r\n",
        "        roi = np.rollaxis(roi, 2, 0)\r\n",
        "        \r\n",
        "    # Match the convolution dimensions\r\n",
        "    roi = np.expand_dims(roi, axis=0)\r\n",
        "    \r\n",
        "    ## Make facial keypoint predictions using your loaded, trained network \r\n",
        "    # Forward pass\r\n",
        "    roi = torch.from_numpy(roi).type(torch.FloatTensor)\r\n",
        "    output_pts = net.forward(roi)\r\n",
        "    \r\n",
        "    output_pts = output_pts.view(output_pts.size()[0], 68, -1)\r\n",
        "    keypoints.append(output_pts[0])\r\n",
        "    \r\n",
        "## Display each detected face and the corresponding keypoints\r\n",
        "show_all_keypoints(images, keypoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbQ5DGbx6eZH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDiuu2Pr6iVp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWU9n97661VJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}